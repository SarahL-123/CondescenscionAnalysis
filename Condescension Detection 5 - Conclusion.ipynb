{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "There were 2 main aims of this project\n",
    "1. Try and quantify what it means to be condescending\n",
    "2. Make a model that can identify whether someone is being condescending.\n",
    "\n",
    "---\n",
    "\n",
    "For the first goal, we were able to find several interesting factors that can help a computer determine what condecenscion is. The first thing that I looked at was what words are commonly found to be condescending. However, this ended up not working as expected because the majority of condescenscion related words are all in the reply. Basically, when someone is condescending, the best way to tell this is if the **reply** contains the word 'condescending' or some kind of swear word/insult.\n",
    "\n",
    "Based on this, we could get a 76% accuracy (for balanced classes) prediction using a logistic regression. However this relies on there being a reply, which is not always possible. Trying to predict condescenscion by only using the original text (no reply) is not accurate at all.\n",
    "\n",
    "---\n",
    "\n",
    "From above, it looks like people who are condescending ended up derailing the conversation. By using BERT to generate embeddings for both the post and the reply, we were able to get a vector representing the topics of each of them. Comparing these vectors shows us a few things:\n",
    "- Posts who are condescending tend to have condescending replies too (note: for the purposes of our predictive model this doesn't matter, this is more aimed towards getting insight into condescenscion)\n",
    "- Condescending posts create a change in the topic of the conversation, this can be seen by plotting the cosine similarity of the topics. Perhaps this could be a way to measure *how* condescending the post is, but a lot more analysis needs to be done to ascertain this.\n",
    "- I found earlier that replies to condescending posts were either 1) insults or 2) commenting on the original post, for example \"Your post was so ___\". Using unsupervised learning I tried to group these replies into several categories but was unable to find groups.\n",
    "- By putting these embeddings into a model we can get about 78% AUC ROC so we can see that just using the embedding works well.\n",
    "\n",
    "Most of this needs a reply to the original post so it's not that useful in all cases. However we were able to find out some information about how people reply to posts that are condescending.\n",
    "\n",
    "---\n",
    "\n",
    "Finally, I tried using BERT + transfer learning to predict condecenscion using only the post. This is more geared towards the second aim of the project, since I would like to detect condescenscion without having to have someone reply.\n",
    "\n",
    "On the other hand, this doesn't really help with the first aim of the project since BERT is a neural net and I can't look at coefficients to see what is going on.\n",
    "\n",
    "In any case this method got an AUC ROC of about 70% (on imbalanced classes although that shouldn't matter). This is a lot better than the original logistic model of only 57% so it's definitely an improvement.\n",
    "\n",
    "---\n",
    "\n",
    "In conclusion, we were able to create a somewhat accurate model for detecting condescenscion without having to have someone reply. If someone does reply, then the model will be significantly more accurate, which is nice but not always possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
