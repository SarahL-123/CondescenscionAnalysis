{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. [Training](#training)\n",
    "2. [Using model](#use_model)\n",
    "\n",
    "---\n",
    " \n",
    "# Train BERT model directly <a id='training'></a>\n",
    "Earlier I tried a lot of models to see if something is condescending, but they all used the reply to the main post. However, I can also use transfer learning with BERT to see if I can just predict by using the post only.\n",
    "\n",
    "Pros:\n",
    "- BERT is a very powerful model\n",
    "- Can look at context of words, which we determined earlier was important\n",
    "- Is NOT limited to only looking at the sentence embeddings, since in the last notebook we only used those.\n",
    "\n",
    "Cons:\n",
    "- Slow to train (it has many parameters)\n",
    "- Can't see what it's using to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quotedpost</th>\n",
       "      <th>quotedreply</th>\n",
       "      <th>label</th>\n",
       "      <th>post</th>\n",
       "      <th>reply</th>\n",
       "      <th>post_user</th>\n",
       "      <th>reply_user</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>end_offset</th>\n",
       "      <th>reddit_post_id</th>\n",
       "      <th>reddit_reply_id</th>\n",
       "      <th>has_cond</th>\n",
       "      <th>post_len</th>\n",
       "      <th>reply_len</th>\n",
       "      <th>cleaned_post</th>\n",
       "      <th>cleaned_reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please educate yoyrself before you bring your ...</td>\n",
       "      <td>Not condescending at all, jeez.</td>\n",
       "      <td>True</td>\n",
       "      <td>Well a guy is saying Barra, who has those grea...</td>\n",
       "      <td>&gt; Please educate yoyrself before you bring you...</td>\n",
       "      <td>StalinHimself</td>\n",
       "      <td>Kel_Casus</td>\n",
       "      <td>135</td>\n",
       "      <td>208</td>\n",
       "      <td>dbl4vl9</td>\n",
       "      <td>dblfraj</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>Well a guy is saying Barra, who has those grea...</td>\n",
       "      <td>Not condescending at all, jeez.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There might be some small piece that's incorrect</td>\n",
       "      <td>You said that. Not me. Not James-Cizuz. You sa...</td>\n",
       "      <td>True</td>\n",
       "      <td>&gt; I think you're the one who has a reading com...</td>\n",
       "      <td>&gt; theories are constantly growing and evolving...</td>\n",
       "      <td>kishi</td>\n",
       "      <td>jids</td>\n",
       "      <td>365</td>\n",
       "      <td>413</td>\n",
       "      <td>c2dtpq9</td>\n",
       "      <td>c2dtywp</td>\n",
       "      <td>1</td>\n",
       "      <td>314</td>\n",
       "      <td>230</td>\n",
       "      <td>Well you're a stupid poopy-head.\\n\\nSee, I don...</td>\n",
       "      <td>Why would theories self-correct if they were a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If I try and force down a breakfast I start ga...</td>\n",
       "      <td>Yes!\\n\\nPeople were so condescending about it ...</td>\n",
       "      <td>False</td>\n",
       "      <td>For me it's like temporarily having the flu. T...</td>\n",
       "      <td>&gt; If I try and force down a breakfast I start ...</td>\n",
       "      <td>amphetaminesfailure</td>\n",
       "      <td>CowGiraffe</td>\n",
       "      <td>331</td>\n",
       "      <td>383</td>\n",
       "      <td>cuv97mf</td>\n",
       "      <td>cuvnb27</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>179</td>\n",
       "      <td>For me it's like temporarily having the flu. T...</td>\n",
       "      <td>Yes!\\n\\nPeople were so condescending about it ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          quotedpost  \\\n",
       "0  Please educate yoyrself before you bring your ...   \n",
       "1   There might be some small piece that's incorrect   \n",
       "2  If I try and force down a breakfast I start ga...   \n",
       "\n",
       "                                         quotedreply  label  \\\n",
       "0                    Not condescending at all, jeez.   True   \n",
       "1  You said that. Not me. Not James-Cizuz. You sa...   True   \n",
       "2  Yes!\\n\\nPeople were so condescending about it ...  False   \n",
       "\n",
       "                                                post  \\\n",
       "0  Well a guy is saying Barra, who has those grea...   \n",
       "1  > I think you're the one who has a reading com...   \n",
       "2  For me it's like temporarily having the flu. T...   \n",
       "\n",
       "                                               reply            post_user  \\\n",
       "0  > Please educate yoyrself before you bring you...        StalinHimself   \n",
       "1  > theories are constantly growing and evolving...                kishi   \n",
       "2  > If I try and force down a breakfast I start ...  amphetaminesfailure   \n",
       "\n",
       "   reply_user  start_offset  end_offset reddit_post_id reddit_reply_id  \\\n",
       "0   Kel_Casus           135         208        dbl4vl9         dblfraj   \n",
       "1        jids           365         413        c2dtpq9         c2dtywp   \n",
       "2  CowGiraffe           331         383        cuv97mf         cuvnb27   \n",
       "\n",
       "   has_cond  post_len  reply_len  \\\n",
       "0         1        37         17   \n",
       "1         1       314        230   \n",
       "2         1       107        179   \n",
       "\n",
       "                                        cleaned_post  \\\n",
       "0  Well a guy is saying Barra, who has those grea...   \n",
       "1  Well you're a stupid poopy-head.\\n\\nSee, I don...   \n",
       "2  For me it's like temporarily having the flu. T...   \n",
       "\n",
       "                                       cleaned_reply  \n",
       "0                    Not condescending at all, jeez.  \n",
       "1  Why would theories self-correct if they were a...  \n",
       "2  Yes!\\n\\nPeople were so condescending about it ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "cond_df = pd.read_csv(\"./cond_data/added_features/balanced_train_more_features.csv\")\n",
    "cond_df.drop(\"Unnamed: 0\", axis = 1, inplace= True)\n",
    "cond_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "- Needs to put text in in a specific format but this can be handled using the tokenizer\n",
    "- Restricted to 512 words/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4961, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get only short posts just to be safe\n",
    "short_posts = cond_df[cond_df[\"post_len\"] < 512]\n",
    "short_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    short_posts[\"cleaned_post\"],\n",
    "    short_posts[\"label\"])\n",
    "\n",
    "# it has to be a 1 and 0, not T or F\n",
    "y_train = [1 if s else 0 for s in list(y_train)]\n",
    "y_val = [1 if s else 0 for s in list(y_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again use the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just tokenizes it for BERT\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoding = tokenizer(list(X_train), truncation = True, padding = True)\n",
    "val_encoding = tokenizer(list(X_val), truncation = True, padding = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the tutorial from https://huggingface.co/transformers/custom_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformer things\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = CondDataset(train_encoding, y_train)\n",
    "val_dataset = CondDataset(val_encoding, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='699' max='699' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [699/699 2:41:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.702091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.694434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.696628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.697723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.695460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.697719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.693809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.686077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.687370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.681903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.658370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.656825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.666447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.640656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.658192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.689283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.678168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.637155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.617702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.677673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.676430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.633360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.664099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.612419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.630727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.564236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.620607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.623611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.594563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.595621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.605447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.617720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.588272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.657529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.654291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.609698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.637651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.575414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.646266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.628392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.622324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.628549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.608905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.597327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.598553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.658691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.629822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.520773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.560168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.530627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.529352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.477597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.514053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.502426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.464084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.489020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.434811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.466287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.521863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.489380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.511215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.443546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.484634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.518210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.475525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.479980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.449649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.489105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.492322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finally, done\n"
     ]
    }
   ],
   "source": [
    "# Continued\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "# uncomment to train\n",
    "# trainer.train()\n",
    "\n",
    "# save the model, uncomment to save\n",
    "# model.save_pretrained(\"models/basic_bert_model_final/\")\n",
    "print(\"Finally done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss seems to be hovering around 0.5 and isn't getting better.\n",
    "\n",
    "# See how well model works <a id='use_model'></a>\n",
    "Load the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"./models/basic_bert_model_final/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that returns the prediction.\n",
    "\n",
    "Also, the model returns a value for each class but it's not a probability, it's just a number. Therefore, just take the larger number and that will be the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentences):\n",
    "    results = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        inputs_encoding = tokenizer.encode(sentence)\n",
    "        \n",
    "        if len(inputs_encoding) > 512:\n",
    "            inputs_encoding = inputs_encoding[:512]\n",
    "        \n",
    "        # see which one is bigger\n",
    "        value0 = model(torch.tensor(inputs_encoding).unsqueeze(0))[0][0][0].data.item()\n",
    "        value1 = model(torch.tensor(inputs_encoding).unsqueeze(0))[0][0][1].data.item()\n",
    "\n",
    "        # return true or false depending if the first or second one is larger\n",
    "        results.append(value0 < value1)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get predictions for validation data\n",
    "val_predictions = predict(list(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it into a number\n",
    "val_predictions_number = [1 if i else 0 for i in val_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, roc_auc_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.8477034649476228\n",
      "AUC ROC:\n",
      "0.8471577650855685\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\")\n",
    "print(accuracy_score(y_val, val_predictions))\n",
    "print(\"AUC ROC:\")\n",
    "print(roc_auc_score(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is actually surprisingly good considering I only used the post and not the response. So this is quite good for predicting condescending posts. However let's look at the confusion matrix too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>0.872240</td>\n",
       "      <td>0.127760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>0.177924</td>\n",
       "      <td>0.822076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0     0.872240     0.127760\n",
       "Actual 1     0.177924     0.822076"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_val, val_predictions, normalize=\"true\")\n",
    "conf_matrix = pd.DataFrame(conf_matrix, columns = [\"Predicted 0\", \"Predicted 1\"], index=[\"Actual 0\", \"Actual 1\"])\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few more false negatives than false positives but not by a lot, so it seems ok. Overall, this model shows that it's possible to predict condescending posts by just looking at the post itself (not the reply).\n",
    "\n",
    "However I don't really know what the model is looking at.\n",
    "\n",
    "Try it on the provided test dataset (completely separate set of data). I wasn't going to do this but since the training used my validation data I wasn't 100% sure if it was fitting on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quotedpost</th>\n",
       "      <th>quotedreply</th>\n",
       "      <th>label</th>\n",
       "      <th>post</th>\n",
       "      <th>reply</th>\n",
       "      <th>post_user</th>\n",
       "      <th>reply_user</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>end_offset</th>\n",
       "      <th>reddit_post_id</th>\n",
       "      <th>reddit_reply_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have seen more biased and tunnel visioned vi...</td>\n",
       "      <td>Challenge them when you see them. Challenge th...</td>\n",
       "      <td>False</td>\n",
       "      <td>I have seen more biased and tunnel visioned vi...</td>\n",
       "      <td>&gt;I have seen more biased and tunnel visioned v...</td>\n",
       "      <td>ykci</td>\n",
       "      <td>kencabbit</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>c0jxu8p</td>\n",
       "      <td>c0jxv1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Poor Trump supporters see people like my famil...</td>\n",
       "      <td>All true, and most of these Trump supporters a...</td>\n",
       "      <td>False</td>\n",
       "      <td>Exactly. Poor Trump supporters see people like...</td>\n",
       "      <td>&gt; Poor Trump supporters see people like my fam...</td>\n",
       "      <td>imrollinv2</td>\n",
       "      <td>michaelochurch</td>\n",
       "      <td>9</td>\n",
       "      <td>230</td>\n",
       "      <td>e3cxi4j</td>\n",
       "      <td>e3d6v8o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love that you blame the casual fans,</td>\n",
       "      <td>Did I ever say that? I actually feel that the ...</td>\n",
       "      <td>True</td>\n",
       "      <td>I love your logic: \"why won't the fans keep co...</td>\n",
       "      <td>&gt;I love your logic: \"why won't the fans keep c...</td>\n",
       "      <td>Skeennn</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>957</td>\n",
       "      <td>995</td>\n",
       "      <td>ck7zo1m</td>\n",
       "      <td>ck802cb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What exactly are you talking about? I suspect ...</td>\n",
       "      <td>Hilarious.  You can't imagine Black voters wou...</td>\n",
       "      <td>True</td>\n",
       "      <td>The point is that you are speaking in vague hy...</td>\n",
       "      <td>&gt; What exactly are you talking about? I suspec...</td>\n",
       "      <td>YabuSama2k</td>\n",
       "      <td>Darrkman</td>\n",
       "      <td>183</td>\n",
       "      <td>301</td>\n",
       "      <td>d5p2erl</td>\n",
       "      <td>d5pd789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>And yet.. you don't see a difference between t...</td>\n",
       "      <td>No, man. I get to respond to your bullshit con...</td>\n",
       "      <td>True</td>\n",
       "      <td>&gt; Bullshit, there's nothing civil in how you s...</td>\n",
       "      <td>&gt;Are you Martin Shkreli? If so.. okay, but if ...</td>\n",
       "      <td>congelar</td>\n",
       "      <td>GroundhogExpert</td>\n",
       "      <td>558</td>\n",
       "      <td>835</td>\n",
       "      <td>dl8afrk</td>\n",
       "      <td>dl8bibd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          quotedpost  \\\n",
       "0  I have seen more biased and tunnel visioned vi...   \n",
       "1  Poor Trump supporters see people like my famil...   \n",
       "2             I love that you blame the casual fans,   \n",
       "3  What exactly are you talking about? I suspect ...   \n",
       "4  And yet.. you don't see a difference between t...   \n",
       "\n",
       "                                         quotedreply  label  \\\n",
       "0  Challenge them when you see them. Challenge th...  False   \n",
       "1  All true, and most of these Trump supporters a...  False   \n",
       "2  Did I ever say that? I actually feel that the ...   True   \n",
       "3  Hilarious.  You can't imagine Black voters wou...   True   \n",
       "4  No, man. I get to respond to your bullshit con...   True   \n",
       "\n",
       "                                                post  \\\n",
       "0  I have seen more biased and tunnel visioned vi...   \n",
       "1  Exactly. Poor Trump supporters see people like...   \n",
       "2  I love your logic: \"why won't the fans keep co...   \n",
       "3  The point is that you are speaking in vague hy...   \n",
       "4  > Bullshit, there's nothing civil in how you s...   \n",
       "\n",
       "                                               reply   post_user  \\\n",
       "0  >I have seen more biased and tunnel visioned v...        ykci   \n",
       "1  > Poor Trump supporters see people like my fam...  imrollinv2   \n",
       "2  >I love your logic: \"why won't the fans keep c...     Skeennn   \n",
       "3  > What exactly are you talking about? I suspec...  YabuSama2k   \n",
       "4  >Are you Martin Shkreli? If so.. okay, but if ...    congelar   \n",
       "\n",
       "        reply_user  start_offset  end_offset reddit_post_id reddit_reply_id  \n",
       "0        kencabbit             0         103        c0jxu8p         c0jxv1l  \n",
       "1   michaelochurch             9         230        e3cxi4j         e3d6v8o  \n",
       "2        [deleted]           957         995        ck7zo1m         ck802cb  \n",
       "3         Darrkman           183         301        d5p2erl         d5pd789  \n",
       "4  GroundhogExpert           558         835        dl8afrk         dl8bibd  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_json('./cond_data/imbalanced_test.jsonl', lines=True)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to remove the quotes like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the post/reply columns but without quotes\n",
    "# this is the same code from the other notebook\n",
    "import regex as re\n",
    "\n",
    "def remove_reddit_quotes(text):\n",
    "    quote_regex = r\">.*\\n\\n\"    \n",
    "    return re.sub(quote_regex, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 65.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data[\"cleaned_post\"] = test_data[\"post\"].map(remove_reddit_quotes)\n",
    "test_data[\"cleaned_reply\"] = test_data[\"reply\"].map(remove_reddit_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, use the model to predict condecenscion using only the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# predict for the test data\n",
    "test_predictions = predict(list(test_data[\"cleaned_post\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ROC:\n",
      "0.7025306748466258\n"
     ]
    }
   ],
   "source": [
    "# imbalanced classes so just use auc roc\n",
    "print(\"AUC ROC:\")\n",
    "print(roc_auc_score(test_data[\"label\"], test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>0.770092</td>\n",
       "      <td>0.229908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>0.365031</td>\n",
       "      <td>0.634969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0     0.770092     0.229908\n",
       "Actual 1     0.365031     0.634969"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(test_data[\"label\"], test_predictions, normalize=\"true\")\n",
    "conf_matrix = pd.DataFrame(conf_matrix, columns = [\"Predicted 0\", \"Predicted 1\"], index=[\"Actual 0\", \"Actual 1\"])\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has over fit on the validation data from earlier as the performance isn't that good. Also there are quite a lot of false negatives (about 35%).\n",
    "\n",
    "Since the aim is to figure out what being condescending is, as well as make a model that can predict people who are condescending, this model isn't that helpful\n",
    "\n",
    "- The accuracy/auc roc is ok but there are still a lot of false negatives\n",
    "- The model is really complicated so it's hard to explain what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
