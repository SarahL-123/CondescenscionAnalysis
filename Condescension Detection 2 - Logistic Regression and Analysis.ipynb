{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. [Intro to this section](#intro)\n",
    "2. [Analysis of Logistic Coefficents](#analysis)\n",
    "3. [Conclusions](#conclusion)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Logistic Model + What is Condecenscion? <a id='intro'></a>\n",
    "\n",
    "As mentioned in the problem statement, one of the goals is to identify common condescending patterns. There are a lot of very powerful models available for NLP but most of them are neural nets and those are hard to understand. Since there isn't a lot of research as to what being condescending is I decided to use a very\n",
    "simple model to do some predictions first.\n",
    "\n",
    "Count vectorizing the words, along with logistic regression, creates a model with easily explainable parameters. If the coefficient is bigger then it's a more condescending word, and if it's smaller it's less.\n",
    "\n",
    "Hopefully with this we can get some insight into things that a model should use to predict condescenscion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quotedpost</th>\n",
       "      <th>quotedreply</th>\n",
       "      <th>label</th>\n",
       "      <th>post</th>\n",
       "      <th>reply</th>\n",
       "      <th>post_user</th>\n",
       "      <th>reply_user</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>end_offset</th>\n",
       "      <th>reddit_post_id</th>\n",
       "      <th>reddit_reply_id</th>\n",
       "      <th>has_cond</th>\n",
       "      <th>post_len</th>\n",
       "      <th>reply_len</th>\n",
       "      <th>cleaned_post</th>\n",
       "      <th>cleaned_reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please educate yoyrself before you bring your ...</td>\n",
       "      <td>Not condescending at all, jeez.</td>\n",
       "      <td>True</td>\n",
       "      <td>Well a guy is saying Barra, who has those grea...</td>\n",
       "      <td>&gt; Please educate yoyrself before you bring you...</td>\n",
       "      <td>StalinHimself</td>\n",
       "      <td>Kel_Casus</td>\n",
       "      <td>135</td>\n",
       "      <td>208</td>\n",
       "      <td>dbl4vl9</td>\n",
       "      <td>dblfraj</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>Well a guy is saying Barra, who has those grea...</td>\n",
       "      <td>Not condescending at all, jeez.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There might be some small piece that's incorrect</td>\n",
       "      <td>You said that. Not me. Not James-Cizuz. You sa...</td>\n",
       "      <td>True</td>\n",
       "      <td>&gt; I think you're the one who has a reading com...</td>\n",
       "      <td>&gt; theories are constantly growing and evolving...</td>\n",
       "      <td>kishi</td>\n",
       "      <td>jids</td>\n",
       "      <td>365</td>\n",
       "      <td>413</td>\n",
       "      <td>c2dtpq9</td>\n",
       "      <td>c2dtywp</td>\n",
       "      <td>1</td>\n",
       "      <td>314</td>\n",
       "      <td>230</td>\n",
       "      <td>Well you're a stupid poopy-head.\\n\\nSee, I don...</td>\n",
       "      <td>Why would theories self-correct if they were a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If I try and force down a breakfast I start ga...</td>\n",
       "      <td>Yes!\\n\\nPeople were so condescending about it ...</td>\n",
       "      <td>False</td>\n",
       "      <td>For me it's like temporarily having the flu. T...</td>\n",
       "      <td>&gt; If I try and force down a breakfast I start ...</td>\n",
       "      <td>amphetaminesfailure</td>\n",
       "      <td>CowGiraffe</td>\n",
       "      <td>331</td>\n",
       "      <td>383</td>\n",
       "      <td>cuv97mf</td>\n",
       "      <td>cuvnb27</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>179</td>\n",
       "      <td>For me it's like temporarily having the flu. T...</td>\n",
       "      <td>Yes!\\n\\nPeople were so condescending about it ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          quotedpost  \\\n",
       "0  Please educate yoyrself before you bring your ...   \n",
       "1   There might be some small piece that's incorrect   \n",
       "2  If I try and force down a breakfast I start ga...   \n",
       "\n",
       "                                         quotedreply  label  \\\n",
       "0                    Not condescending at all, jeez.   True   \n",
       "1  You said that. Not me. Not James-Cizuz. You sa...   True   \n",
       "2  Yes!\\n\\nPeople were so condescending about it ...  False   \n",
       "\n",
       "                                                post  \\\n",
       "0  Well a guy is saying Barra, who has those grea...   \n",
       "1  > I think you're the one who has a reading com...   \n",
       "2  For me it's like temporarily having the flu. T...   \n",
       "\n",
       "                                               reply            post_user  \\\n",
       "0  > Please educate yoyrself before you bring you...        StalinHimself   \n",
       "1  > theories are constantly growing and evolving...                kishi   \n",
       "2  > If I try and force down a breakfast I start ...  amphetaminesfailure   \n",
       "\n",
       "   reply_user  start_offset  end_offset reddit_post_id reddit_reply_id  \\\n",
       "0   Kel_Casus           135         208        dbl4vl9         dblfraj   \n",
       "1        jids           365         413        c2dtpq9         c2dtywp   \n",
       "2  CowGiraffe           331         383        cuv97mf         cuvnb27   \n",
       "\n",
       "   has_cond  post_len  reply_len  \\\n",
       "0         1        37         17   \n",
       "1         1       314        230   \n",
       "2         1       107        179   \n",
       "\n",
       "                                        cleaned_post  \\\n",
       "0  Well a guy is saying Barra, who has those grea...   \n",
       "1  Well you're a stupid poopy-head.\\n\\nSee, I don...   \n",
       "2  For me it's like temporarily having the flu. T...   \n",
       "\n",
       "                                       cleaned_reply  \n",
       "0                    Not condescending at all, jeez.  \n",
       "1  Why would theories self-correct if they were a...  \n",
       "2  Yes!\\n\\nPeople were so condescending about it ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data (that we cleaned in the prev notebook)\n",
    "cond_df = pd.read_csv(\"./cond_data/added_features/balanced_train_more_features.csv\")\n",
    "cond_df.drop(\"Unnamed: 0\", axis = 1, inplace=True)\n",
    "cond_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to look at both the post and the reply, so I need to combine both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################\n",
    "# Some code from the prev notebook\n",
    "##################################\n",
    "\n",
    "# It's just wrappers for the word stemmer and todense() functions\n",
    "# so I can put them into a pipeline\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "class BaseClass:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    # class must implement fit and xform\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "class StemOrLemmatizer(BaseClass):\n",
    "    \n",
    "    def __init__(self, cols, choice = 'stem'):\n",
    "        \n",
    "        self._return_df = True\n",
    "        if isinstance(cols, str):\n",
    "            cols = [cols]\n",
    "        if len(cols) == 1:\n",
    "            self._return_df = False\n",
    "        \n",
    "        # It will only lemmatize the columns that you specify here\n",
    "        self.cols_to_encode = cols\n",
    "        \n",
    "        if choice not in [\"stem\", \"lemma\"]:\n",
    "            raise Exception(\"choice parameter can only be 'stem' or 'lemma'\")\n",
    "        self.choice = choice\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        # create list of lists\n",
    "        # the outer list is basically each column\n",
    "        # the inner list is the entries in each column\n",
    "        list_of_lists = []\n",
    "        \n",
    "        if self.choice == \"stem\":\n",
    "            lemma = PorterStemmer()\n",
    "        else:\n",
    "            lemma = WordNetLemmatizer()\n",
    "        \n",
    "        #loop through all columns\n",
    "        for i, col_name in enumerate(self.cols_to_encode):\n",
    "            # add a new list (i.e. a new column)\n",
    "            list_of_lists.append([])\n",
    "            \n",
    "            # loop through each column and append to list\n",
    "            # not the most computationally efficient but w/e\n",
    "            for sentence in X[col_name]:\n",
    "                # loop through each word and lemmatize/stem it\n",
    "                split_words = sentence.split()\n",
    "                \n",
    "                if self.choice == \"stem\":\n",
    "                    split_words = [lemma.stem(s) for s in split_words]\n",
    "                else:\n",
    "                    split_words = [lemma.lemmatize(s) for s in split_words]\n",
    "                    \n",
    "                new_text = \" \".join(split_words)\n",
    "                \n",
    "                # save this to the list\n",
    "                list_of_lists[i].append(new_text)\n",
    "                \n",
    "        # if you only have one column, it should return a series (not DF)\n",
    "        # this is to allow it to pass directly into TFIDF without causing errors\n",
    "        if self._return_df == False:\n",
    "            return pd.Series(list_of_lists[0])\n",
    "        \n",
    "        # well turns out my list was the wrong way, so just transpose it\n",
    "        return pd.DataFrame(list_of_lists).transpose()\n",
    "\n",
    "# and a class that dense-s it after vectorizing\n",
    "class ToDense(BaseClass):\n",
    "    def transform(self, data):\n",
    "        return data.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import more model things\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# its useful later\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a n-gram range of 2 because I want to know if there are any condescending phrases, not just specific words. 3 would take too long so I just used 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar pipeline to the previous notebook (uses different vectorizer and classifier)\n",
    "# For both the post and reply, stem > vectorize > convert to dense matrix\n",
    "# then, combine both and feed into logistic regression\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# for posts\n",
    "post_pipeline_steps = [(\"stem\", StemOrLemmatizer(cols = [\"cleaned_post\"])), # we'll still stem it\n",
    "               (\"cvec\", CountVectorizer(stop_words = ENGLISH_STOP_WORDS, min_df = 10, ngram_range=(1,2))), # use cvect for easy interpretation\n",
    "               (\"dense\", ToDense())] # still need this\n",
    "\n",
    "# for replies, it's basically the same so make a copy\n",
    "# (we want different objs so make deep copy)\n",
    "reply_pipeline_steps = copy.deepcopy(post_pipeline_steps)\n",
    "\n",
    "# it uses a different col so just change that\n",
    "reply_pipeline_steps[0] = (\"stem\", StemOrLemmatizer(cols = [\"cleaned_reply\"]))\n",
    "\n",
    "\n",
    "# make pipelines\n",
    "post_pipeline = Pipeline(post_pipeline_steps)\n",
    "reply_pipeline = Pipeline(reply_pipeline_steps)\n",
    "\n",
    "# combine the 2 individual pipelines\n",
    "combined = FeatureUnion([(\"post\", post_pipeline),\n",
    "                         (\"reply\", reply_pipeline)])\n",
    "\n",
    "# add in the logistic regression to the end\n",
    "logreg_pipe = Pipeline([(\"combine\", combined),\n",
    "                        (\"logreg\", LogisticRegression(max_iter = 1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(cond_df.drop(\"label\", axis = 1),\n",
    "                                                    cond_df[\"label\"],\n",
    "                                                    random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = logreg_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7311827956989247"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how well it did on the test data\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999231950844854"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how well it did on the training data\n",
    "accuracy_score(y_train, logreg_pipe.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually this is quite promising since the accuracy is high (better than the previous model). Although I have to tune it (regularize the logistic regression), but that shouldn't affect the explainability of the model since it just affects the coefficients.\n",
    "\n",
    "It's pretty fast to do a grid search so let's just do one right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\"logreg__C\" : [1/i for i in np.linspace(1, 10, 4)],\n",
    "                 \"logreg__l1_ratio\" : np.linspace(0, 1, 3),\n",
    "                 \"logreg__penalty\" : [\"elasticnet\"],\n",
    "                 \"logreg__solver\" : [\"saga\"]\n",
    "                 }\n",
    "\n",
    "gridsearch = GridSearchCV(logreg_pipe, search_params, n_jobs=-1, cv = 4, verbose =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 26.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "gridsearch.fit(X_train, y_train)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('combine',\n",
       "                 FeatureUnion(transformer_list=[('post',\n",
       "                                                 Pipeline(steps=[('stem',\n",
       "                                                                  <__main__.StemOrLemmatizer object at 0x00000216482B2130>),\n",
       "                                                                 ('cvec',\n",
       "                                                                  CountVectorizer(min_df=10,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  stop_words=frozenset({'a',\n",
       "                                                                                                        'about',\n",
       "                                                                                                        'above',\n",
       "                                                                                                        'across',\n",
       "                                                                                                        'after',\n",
       "                                                                                                        'afterwards',\n",
       "                                                                                                        'again',\n",
       "                                                                                                        'against',\n",
       "                                                                                                        'all',\n",
       "                                                                                                        'almost',\n",
       "                                                                                                        'alone',\n",
       "                                                                                                        'along',\n",
       "                                                                                                        'already',\n",
       "                                                                                                        'also',\n",
       "                                                                                                        'although',\n",
       "                                                                                                        'alw...\n",
       "                                                                                                        'alone',\n",
       "                                                                                                        'along',\n",
       "                                                                                                        'already',\n",
       "                                                                                                        'also',\n",
       "                                                                                                        'although',\n",
       "                                                                                                        'always',\n",
       "                                                                                                        'am',\n",
       "                                                                                                        'among',\n",
       "                                                                                                        'amongst',\n",
       "                                                                                                        'amoungst',\n",
       "                                                                                                        'amount',\n",
       "                                                                                                        'an',\n",
       "                                                                                                        'and',\n",
       "                                                                                                        'another',\n",
       "                                                                                                        'any',\n",
       "                                                                                                        'anyhow',\n",
       "                                                                                                        'anyone',\n",
       "                                                                                                        'anything',\n",
       "                                                                                                        'anyway',\n",
       "                                                                                                        'anywhere', ...}))),\n",
       "                                                                 ('dense',\n",
       "                                                                  <__main__.ToDense object at 0x00000216484BD610>)]))])),\n",
       "                ('logreg',\n",
       "                 LogisticRegression(C=0.14285714285714285, l1_ratio=1.0,\n",
       "                                    max_iter=1000, penalty='elasticnet',\n",
       "                                    solver='saga'))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the best logistic regression is elastic net with C = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631791449234022"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = gridsearch.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7765880132137801"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC AUC/accuracy is pretty good too. So now that we have a decently good model I want to look at what the model is using to predict the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best model\n",
    "best_logistic_model = gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this model, get the coeffs of the logistic regression\n",
    "logistic_coeffs = best_logistic_model.named_steps[\"logreg\"].coef_\n",
    "\n",
    "# get the words from the post\n",
    "post_vocab = dict(best_logistic_model.named_steps[\"combine\"].transformer_list) \\\n",
    "                .get(\"post\").named_steps[\"cvec\"].get_feature_names()\n",
    "\n",
    "# words from reply\n",
    "reply_vocab = dict(best_logistic_model.named_steps[\"combine\"].transformer_list) \\\n",
    "                .get(\"reply\").named_steps[\"cvec\"].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a combined list so I can put it into a big dataframe\n",
    "all_vocab = copy.deepcopy(post_vocab)\n",
    "all_vocab.extend(reply_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need a column that says whether the word is initially from post or reply\n",
    "# since words are likely to be repeated between the 2\n",
    "original_source = [1] * len(post_vocab)\n",
    "original_source.extend([0] * len(reply_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into dataframe so its easier to use\n",
    "logistic_results = pd.DataFrame([all_vocab, logistic_coeffs[0], original_source]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename cols\n",
    "logistic_results.rename({0 : \"Word\",\n",
    "                         1 : \"Logistic coefficient\",\n",
    "                         2 : \"Word from post\"}, axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to sort by magnitude so just put a column that is the absolute value\n",
    "logistic_results[\"abs_coeff\"] = np.abs(logistic_results[\"Logistic coefficient\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Logistic coefficient</th>\n",
       "      <th>Word from post</th>\n",
       "      <th>abs_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4209</th>\n",
       "      <td>condescend</td>\n",
       "      <td>2.92917</td>\n",
       "      <td>0</td>\n",
       "      <td>2.92917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4242</th>\n",
       "      <td>condescending</td>\n",
       "      <td>2.91712</td>\n",
       "      <td>0</td>\n",
       "      <td>2.91712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>condescendingli</td>\n",
       "      <td>2.19894</td>\n",
       "      <td>0</td>\n",
       "      <td>2.19894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4569</th>\n",
       "      <td>don mean</td>\n",
       "      <td>-1.17573</td>\n",
       "      <td>0</td>\n",
       "      <td>1.17573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6648</th>\n",
       "      <td>sound condescending</td>\n",
       "      <td>-0.904462</td>\n",
       "      <td>0</td>\n",
       "      <td>0.904462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>condescens</td>\n",
       "      <td>-0.8053</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5751</th>\n",
       "      <td>need condescending</td>\n",
       "      <td>0.716781</td>\n",
       "      <td>0</td>\n",
       "      <td>0.716781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6711</th>\n",
       "      <td>stay</td>\n",
       "      <td>-0.542961</td>\n",
       "      <td>0</td>\n",
       "      <td>0.542961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7190</th>\n",
       "      <td>wa condescending</td>\n",
       "      <td>-0.525116</td>\n",
       "      <td>0</td>\n",
       "      <td>0.525116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>took</td>\n",
       "      <td>-0.446282</td>\n",
       "      <td>0</td>\n",
       "      <td>0.446282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4919</th>\n",
       "      <td>fuck</td>\n",
       "      <td>0.394521</td>\n",
       "      <td>0</td>\n",
       "      <td>0.394521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>sell</td>\n",
       "      <td>0.378128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.378128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>lol</td>\n",
       "      <td>0.372392</td>\n",
       "      <td>1</td>\n",
       "      <td>0.372392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3858</th>\n",
       "      <td>ban</td>\n",
       "      <td>-0.369967</td>\n",
       "      <td>0</td>\n",
       "      <td>0.369967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>lot</td>\n",
       "      <td>-0.363968</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>concern</td>\n",
       "      <td>-0.356682</td>\n",
       "      <td>1</td>\n",
       "      <td>0.356682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>tone</td>\n",
       "      <td>-0.349949</td>\n",
       "      <td>1</td>\n",
       "      <td>0.349949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>obviou</td>\n",
       "      <td>0.349915</td>\n",
       "      <td>1</td>\n",
       "      <td>0.349915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5965</th>\n",
       "      <td>perhap</td>\n",
       "      <td>-0.323061</td>\n",
       "      <td>0</td>\n",
       "      <td>0.323061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>accus</td>\n",
       "      <td>-0.314644</td>\n",
       "      <td>0</td>\n",
       "      <td>0.314644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>condescend prick</td>\n",
       "      <td>0.304641</td>\n",
       "      <td>0</td>\n",
       "      <td>0.304641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6752</th>\n",
       "      <td>stuff</td>\n",
       "      <td>-0.298368</td>\n",
       "      <td>0</td>\n",
       "      <td>0.298368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>thank</td>\n",
       "      <td>-0.298006</td>\n",
       "      <td>1</td>\n",
       "      <td>0.298006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>speak</td>\n",
       "      <td>-0.293858</td>\n",
       "      <td>1</td>\n",
       "      <td>0.293858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>wasn</td>\n",
       "      <td>-0.280564</td>\n",
       "      <td>0</td>\n",
       "      <td>0.280564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>small</td>\n",
       "      <td>-0.278546</td>\n",
       "      <td>1</td>\n",
       "      <td>0.278546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>meat</td>\n",
       "      <td>-0.261723</td>\n",
       "      <td>1</td>\n",
       "      <td>0.261723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6483</th>\n",
       "      <td>seen</td>\n",
       "      <td>-0.257556</td>\n",
       "      <td>0</td>\n",
       "      <td>0.257556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>prove</td>\n",
       "      <td>0.254539</td>\n",
       "      <td>0</td>\n",
       "      <td>0.254539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>develop</td>\n",
       "      <td>0.247893</td>\n",
       "      <td>1</td>\n",
       "      <td>0.247893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2794</th>\n",
       "      <td>short</td>\n",
       "      <td>-0.239596</td>\n",
       "      <td>1</td>\n",
       "      <td>0.239596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>agree</td>\n",
       "      <td>-0.23867</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>mainstream</td>\n",
       "      <td>-0.238636</td>\n",
       "      <td>0</td>\n",
       "      <td>0.238636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>group</td>\n",
       "      <td>-0.236297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>live</td>\n",
       "      <td>0.229683</td>\n",
       "      <td>1</td>\n",
       "      <td>0.229683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>audienc</td>\n",
       "      <td>-0.224466</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6849</th>\n",
       "      <td>thank</td>\n",
       "      <td>0.223768</td>\n",
       "      <td>0</td>\n",
       "      <td>0.223768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4168</th>\n",
       "      <td>comment</td>\n",
       "      <td>0.217514</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>dont</td>\n",
       "      <td>0.216813</td>\n",
       "      <td>1</td>\n",
       "      <td>0.216813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>understand</td>\n",
       "      <td>0.209948</td>\n",
       "      <td>1</td>\n",
       "      <td>0.209948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6515</th>\n",
       "      <td>sexist</td>\n",
       "      <td>-0.205817</td>\n",
       "      <td>0</td>\n",
       "      <td>0.205817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6405</th>\n",
       "      <td>said</td>\n",
       "      <td>0.204792</td>\n",
       "      <td>0</td>\n",
       "      <td>0.204792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3693</th>\n",
       "      <td>alway</td>\n",
       "      <td>-0.204328</td>\n",
       "      <td>0</td>\n",
       "      <td>0.204328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>posit</td>\n",
       "      <td>-0.202512</td>\n",
       "      <td>1</td>\n",
       "      <td>0.202512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6365</th>\n",
       "      <td>right</td>\n",
       "      <td>0.191947</td>\n",
       "      <td>0</td>\n",
       "      <td>0.191947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3838</th>\n",
       "      <td>averag</td>\n",
       "      <td>0.187672</td>\n",
       "      <td>0</td>\n",
       "      <td>0.187672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>apolog</td>\n",
       "      <td>-0.186295</td>\n",
       "      <td>0</td>\n",
       "      <td>0.186295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5211</th>\n",
       "      <td>incred</td>\n",
       "      <td>0.185011</td>\n",
       "      <td>0</td>\n",
       "      <td>0.185011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>art</td>\n",
       "      <td>-0.18346</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>read</td>\n",
       "      <td>0.17815</td>\n",
       "      <td>1</td>\n",
       "      <td>0.17815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Word Logistic coefficient Word from post abs_coeff\n",
       "4209           condescend              2.92917              0   2.92917\n",
       "4242        condescending              2.91712              0   2.91712\n",
       "4251      condescendingli              2.19894              0   2.19894\n",
       "4569             don mean             -1.17573              0   1.17573\n",
       "6648  sound condescending            -0.904462              0  0.904462\n",
       "617            condescens              -0.8053              1    0.8053\n",
       "5751   need condescending             0.716781              0  0.716781\n",
       "6711                 stay            -0.542961              0  0.542961\n",
       "7190     wa condescending            -0.525116              0  0.525116\n",
       "7005                 took            -0.446282              0  0.446282\n",
       "4919                 fuck             0.394521              0  0.394521\n",
       "2750                 sell             0.378128              1  0.378128\n",
       "1855                  lol             0.372392              1  0.372392\n",
       "3858                  ban            -0.369967              0  0.369967\n",
       "5533                  lot            -0.363968              0  0.363968\n",
       "610               concern            -0.356682              1  0.356682\n",
       "3216                 tone            -0.349949              1  0.349949\n",
       "2122               obviou             0.349915              1  0.349915\n",
       "5965               perhap            -0.323061              0  0.323061\n",
       "3618                accus            -0.314644              0  0.314644\n",
       "4228     condescend prick             0.304641              0  0.304641\n",
       "6752                stuff            -0.298368              0  0.298368\n",
       "3084                thank            -0.298006              1  0.298006\n",
       "2906                speak            -0.293858              1  0.293858\n",
       "7237                 wasn            -0.280564              0  0.280564\n",
       "2848                small            -0.278546              1  0.278546\n",
       "1963                 meat            -0.261723              1  0.261723\n",
       "6483                 seen            -0.257556              0  0.257556\n",
       "6171                prove             0.254539              0  0.254539\n",
       "798               develop             0.247893              1  0.247893\n",
       "2794                short            -0.239596              1  0.239596\n",
       "3668                agree             -0.23867              0   0.23867\n",
       "5549           mainstream            -0.238636              0  0.238636\n",
       "5011                group            -0.236297              0  0.236297\n",
       "1838                 live             0.229683              1  0.229683\n",
       "3832              audienc            -0.224466              0  0.224466\n",
       "6849                thank             0.223768              0  0.223768\n",
       "4168              comment             0.217514              0  0.217514\n",
       "930                  dont             0.216813              1  0.216813\n",
       "3290           understand             0.209948              1  0.209948\n",
       "6515               sexist            -0.205817              0  0.205817\n",
       "6405                 said             0.204792              0  0.204792\n",
       "3693                alway            -0.204328              0  0.204328\n",
       "2363                posit            -0.202512              1  0.202512\n",
       "6365                right             0.191947              0  0.191947\n",
       "3838               averag             0.187672              0  0.187672\n",
       "3746               apolog            -0.186295              0  0.186295\n",
       "5211               incred             0.185011              0  0.185011\n",
       "3782                  art             -0.18346              0   0.18346\n",
       "2517                 read              0.17815              1   0.17815"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the top most effective words for classification\n",
    "logistic_results.sort_values(\"abs_coeff\", ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of coefficients <a id='analysis'></a>\n",
    "\n",
    "As expected the top words are just different tenses of the word \"condescending\". The stemmer didn't really work that well since they ended up as different words but as we have a lot of data it was still ok.\n",
    "\n",
    "**The most interesting thing to note is**: the most useful words that the model is using to predict the outcome are ones in the reply. In a sense, by looking at how people respond, we can determine whether the initial post is condescending or not. This means that the model isn't predicting condescenscion in general but rather what the sentiment of the reply is.\n",
    "\n",
    "This is bad if we want to make a model that identifies condescending Reddit posts, since it only works if someone replies (although once they reply it becomes quite accurate).\n",
    "\n",
    "It also means for more complicated models, I should try some kind of sentiment analysis on the replies.\n",
    "\n",
    "---\n",
    "\n",
    "These are the words from the **reply** that the model will use to predict, along with a '+' or '-' for whether the word is associated with condescenscion\n",
    "- Various tenses of the word 'condescending' (+)\n",
    "- \"don't mean\" (-)\n",
    "- \"stay\" (-)\n",
    "- \"took\" (-)\n",
    "- \"f\\*\\*k\" (+)\n",
    "- \"ban\" (-)\n",
    "- \"lot\" (-)\n",
    "    \n",
    "Thus we can see that replies to a condescending post fall into these categories:\n",
    "\n",
    "1. uses strong language to reply\n",
    "2. Points out something in the original post itself, e.g: 'tone', 'comment', 'assumption'\n",
    "\n",
    "And the replies to a condescending post don't use the words shown above (with a '-' sign) although I don't really know what the pattern is. \n",
    "\n",
    "---\n",
    "\n",
    "In addition, the best words for prediction from the **post** are:\n",
    "- \"condescending\" (-) (if this word is present in the post, it means it is much **less** likely to be condescending\")\n",
    "- \"sell\" (+)\n",
    "- \"lol\" (+)\n",
    "- \"concern\" (-)\n",
    "- \"tone\" (-)\n",
    "- \"thank\" (-)\n",
    "- \"speak (-)\n",
    "\n",
    "The most obvious thing about this is that if someone says the word condescending, they are probably trying not to be condescending, which kind of makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification from only the post\n",
    "From the above we can see that a model will likely predict the outcome using mostly the reply. I want to know if we can just look at the post for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just try a logistic model on the posts only and see what happens. I know in the first notebook I already did something like this, but I didn't analyze the coefficients (which is really what we are going for here) and also logistic regression works better than the naive bayes classifier used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a pipeline again\n",
    "post_only_steps = [(\"stem\", StemOrLemmatizer(cols=[\"cleaned_post\"])),\n",
    "                   (\"cvec\", CountVectorizer(stop_words=ENGLISH_STOP_WORDS, min_df = 10, ngram_range=(1,2))),\n",
    "                   (\"dense\", ToDense()),\n",
    "                   (\"logistic\", LogisticRegression(penalty=\"elasticnet\",\n",
    "                                                   C=0.1,\n",
    "                                                   solver=\"saga\",\n",
    "                                                   max_iter=10000))]\n",
    "                    # ^ use the same parameters we found earlier\n",
    "\n",
    "post_only_pipeline = Pipeline(post_only_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a grid search\n",
    "grid_params = {\"logistic__l1_ratio\" : [0.3, 0.4, 0.5]}\n",
    "\n",
    "post_gridsearch = GridSearchCV(post_only_pipeline, grid_params, n_jobs = -1, cv = 4, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:  1.9min remaining:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4,\n",
       "             estimator=Pipeline(steps=[('stem',\n",
       "                                        <__main__.StemOrLemmatizer object at 0x0000021649AB61C0>),\n",
       "                                       ('cvec',\n",
       "                                        CountVectorizer(min_df=10,\n",
       "                                                        ngram_range=(1, 2),\n",
       "                                                        stop_words=frozenset({'a',\n",
       "                                                                              'about',\n",
       "                                                                              'above',\n",
       "                                                                              'across',\n",
       "                                                                              'after',\n",
       "                                                                              'afterwards',\n",
       "                                                                              'again',\n",
       "                                                                              'against',\n",
       "                                                                              'all',\n",
       "                                                                              'almost',\n",
       "                                                                              'alone',\n",
       "                                                                              'along',\n",
       "                                                                              'already',\n",
       "                                                                              'also',\n",
       "                                                                              'although',\n",
       "                                                                              'always',\n",
       "                                                                              'am',\n",
       "                                                                              'among',\n",
       "                                                                              'amongst',\n",
       "                                                                              'amoungst',\n",
       "                                                                              'amount',\n",
       "                                                                              'an',\n",
       "                                                                              'and',\n",
       "                                                                              'another',\n",
       "                                                                              'any',\n",
       "                                                                              'anyhow',\n",
       "                                                                              'anyone',\n",
       "                                                                              'anything',\n",
       "                                                                              'anyway',\n",
       "                                                                              'anywhere', ...}))),\n",
       "                                       ('dense',\n",
       "                                        <__main__.ToDense object at 0x0000021649AB6460>),\n",
       "                                       ('logistic',\n",
       "                                        LogisticRegression(C=0.1,\n",
       "                                                           max_iter=10000,\n",
       "                                                           penalty='elasticnet',\n",
       "                                                           solver='saga'))]),\n",
       "             n_jobs=-1, param_grid={'logistic__l1_ratio': [0.3, 0.4, 0.5]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how well it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5770736677181737"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_gridsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = post_gridsearch.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5705663048607834"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy and AUC ROC is really bad (as expected).\n",
    "\n",
    "I'll just look at the coeffs anyway, since I already got to this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_only_best_model = post_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coefficients of logistic regression\n",
    "logistic_coeffs = posts_only_best_model.named_steps[\"logistic\"].coef_\n",
    "\n",
    "# get the words used to vectorize\n",
    "post_vocab = posts_only_best_model.named_steps[\"cvec\"].get_feature_names()\n",
    "\n",
    "# put it into a big DF\n",
    "post_only_words_df = pd.DataFrame([post_vocab, logistic_coeffs[0]]).transpose()\n",
    "\n",
    "# change the column names\n",
    "post_only_words_df.rename({0 : \"word\", 1: \"Logistic Coeff\"}, axis = 1, inplace=True)\n",
    "\n",
    "post_only_words_df[\"abs_coeff\"] = np.abs(post_only_words_df[\"Logistic Coeff\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Logistic Coeff</th>\n",
       "      <th>abs_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2794</th>\n",
       "      <td>short</td>\n",
       "      <td>-0.526347</td>\n",
       "      <td>0.526347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3084</th>\n",
       "      <td>thank</td>\n",
       "      <td>-0.42569</td>\n",
       "      <td>0.42569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>lol</td>\n",
       "      <td>0.358458</td>\n",
       "      <td>0.358458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>sell</td>\n",
       "      <td>0.354544</td>\n",
       "      <td>0.354544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>concern</td>\n",
       "      <td>-0.346834</td>\n",
       "      <td>0.346834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>outright</td>\n",
       "      <td>-0.339343</td>\n",
       "      <td>0.339343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2122</th>\n",
       "      <td>obviou</td>\n",
       "      <td>0.331344</td>\n",
       "      <td>0.331344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>luck</td>\n",
       "      <td>0.32594</td>\n",
       "      <td>0.32594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>downvot</td>\n",
       "      <td>0.292981</td>\n",
       "      <td>0.292981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>elect</td>\n",
       "      <td>0.292696</td>\n",
       "      <td>0.292696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>understand</td>\n",
       "      <td>0.292021</td>\n",
       "      <td>0.292021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>serv</td>\n",
       "      <td>-0.275326</td>\n",
       "      <td>0.275326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3560</th>\n",
       "      <td>young</td>\n",
       "      <td>0.273868</td>\n",
       "      <td>0.273868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>ignor</td>\n",
       "      <td>0.273179</td>\n",
       "      <td>0.273179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>stuff</td>\n",
       "      <td>-0.268291</td>\n",
       "      <td>0.268291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>arm</td>\n",
       "      <td>-0.26595</td>\n",
       "      <td>0.26595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3549</th>\n",
       "      <td>www</td>\n",
       "      <td>-0.257894</td>\n",
       "      <td>0.257894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>meat</td>\n",
       "      <td>-0.257057</td>\n",
       "      <td>0.257057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>link</td>\n",
       "      <td>0.250995</td>\n",
       "      <td>0.250995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2949</th>\n",
       "      <td>stat</td>\n",
       "      <td>0.250787</td>\n",
       "      <td>0.250787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word Logistic Coeff abs_coeff\n",
       "2794       short      -0.526347  0.526347\n",
       "3084       thank       -0.42569   0.42569\n",
       "1855         lol       0.358458  0.358458\n",
       "2750        sell       0.354544  0.354544\n",
       "610      concern      -0.346834  0.346834\n",
       "2182    outright      -0.339343  0.339343\n",
       "2122      obviou       0.331344  0.331344\n",
       "1878        luck        0.32594   0.32594\n",
       "935      downvot       0.292981  0.292981\n",
       "987        elect       0.292696  0.292696\n",
       "3290  understand       0.292021  0.292021\n",
       "2765        serv      -0.275326  0.275326\n",
       "3560       young       0.273868  0.273868\n",
       "1509       ignor       0.273179  0.273179\n",
       "2997       stuff      -0.268291  0.268291\n",
       "211          arm       -0.26595   0.26595\n",
       "3549         www      -0.257894  0.257894\n",
       "1963        meat      -0.257057  0.257057\n",
       "1829        link       0.250995  0.250995\n",
       "2949        stat       0.250787  0.250787"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_only_words_df.sort_values(by = \"abs_coeff\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit hard to see what the pattern is for this. I tried grouping the words by which category they were in:\n",
    "\n",
    "Words that are more condescending:\n",
    "- lol\n",
    "- sell\n",
    "- obvious\n",
    "- luck\n",
    "- downvote\n",
    "- elect\n",
    "- understand\n",
    "\n",
    "Words that are less condescending:\n",
    "- short\n",
    "- thank\n",
    "- concern\n",
    "- outright\n",
    "- serve\n",
    "\n",
    "(Note that I only vectorized words with >=10 total occurrences)\n",
    "\n",
    "I can't really tell if there is any pattern here and since the accuracy is low, I don't think it's worth the time to look at this more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion for this notebook <a id='conclusion'></a>\n",
    "In this notebook we found a few important things\n",
    "\n",
    "- Predicting whether a post is condescending or not mostly depends on the reply and not the post itself. Partly it's because our model is kind of simple.\n",
    "    - If you try using just the post to predict, you probably need a more advanced model since logistic regression isn't going to work.\n",
    "- When someone is condescending, the replies will usually be one of these two:\n",
    "    - Using strong language (e.g: \"don't be a prick\")\n",
    "    - Refers to something in the original post (e.g: \"your post is ___\")\n",
    "- People who mention the word 'condescending' are trying not to be condescending.\n",
    "\n",
    "# What next?\n",
    "Since replies to condescending posts seem to fall into 2 categories/topics, I want to try and model this change of topic. Please see the next notebook for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
